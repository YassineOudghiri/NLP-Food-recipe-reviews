---
title: "Final Group Project - Text Analysis for Business "
author: "Authors: Aditya Moudgil; Henrique Moreira; Idil Zorlu; Jacop Kachel; Yassine Drissi Oudghiri"
date: "`r Sys.Date()`"
output: html_document
---

This group project intends to ... (INTRODUCTION)

## Table of Contents

1.  Importing and preparing the data

2.  Exploratory data analysis

    2.1. Research quesiton

3.  Apllying unigrams & bigrams and bigrams & trigrams stemming to the users' reviews to predict recipes' ratings

4.  Aplying unigrams & bigrams stemming to users' reviews and to recipes' descriptions, steps and ingredients to predict recipes' ratings

5.  Benchmarks

6.  Predicting if a user is going to write a positive or a negative review (same approach as before)

7.  Transfer learning - training a model on positive reviews to predict negative reviews and vice-versa

8.  Politeness on Review Data

9.  Limitations and improvements

## 1. Importing and preparing the data

The first step is to load the necessary libraries for the analysis.

```{r message=FALSE}
# Libraries
library(dplyr)
library(tidyverse)
library(corrplot)
library(ggplot2)
library(quanteda)
library(ggrepel)
library(textclean)
library(glmnet)
library(sentimentr)
library(spacyr) 
library(politeness)
library(data.table)
library(caret)
library(text2vec)
library(tidytext)
library(textdata)
library(wordcloud)

```

Let's save everyone's setwd here and just remove the hashtag for yours when you need it (we'll delete this when it's time to submit)

```{r}
#setwd("/Users/henriquemoreira/Desktop/MSBA/2nd term/Text Analysis for Business/Final Group Project")
#setwd('/Users/idilzorlu/Desktop/TA/GA/recipies')
setwd("/Users/jacobkachel/Documents/Imperial/Modules/Text Analysis for Business/R - Text Analysis for Business/Group Project")
```

We also load pre-written functions and RDS files that will be useful for analysing text.

```{r}
# Also place auxiliary functions "TAB_dfm.R" and RDS files like "vecSmall.RDS" here:
source("vectorFunctions.R") 
source("TAB_dfm.R")
source("kendall_acc.R")
vecSmall<-readRDS("vecSmall.RDS")
load("wfFile.RData")
```

At last, we import the two data-sets that we're going to focus on: "RAW_recipes", which contains several information about "food.com" recipes and "RAW_interactions", which contains information about "food.com" users' reviews of the recipes.

```{r}
# Loading the two data-sets
raw_recipes <- read.csv("data/RAW_recipes.csv", stringsAsFactors = FALSE)
interactions <- read.csv("data/RAW_interactions.csv", stringsAsFactors = FALSE)
# Let's merge
merged_data <- left_join(interactions, raw_recipes, by = c("recipe_id" = "id"))
```

## 2. Exploratory Data Analysis

Let's take a quick look at the data.

```{r}
head(merged_data, 1) # Restricting to one row because the columns containing text make the output message too big
# Checking the class type of each attribute
sapply(merged_data, class)
# Checking how many rows the data-set has
cat("The merged data-set contains", nrow(merged_data), "rows")
```

The merged data-set therefore consists of 1.132.367 rows and 16 variables - 7 integer and 9 character.

We can see that the column "date" is in character format so we'll change it to date format.

```{r}
merged_data$date <- as.Date(merged_data$date)
```

Let's check if there are any NA values in the data-set.

```{r}
cat("The merged data-set has", sum(is.na(merged_data)), "NA values")
```

For better-understanding purposes, the following is a description of each variable present in the data-set:

-   **user_id** -\> User ID

-   **recipe_id** -\> Recipe ID

-   **date** -\> Date of interaction

-   **rating** -\> Rating given by a user

-   **review** -\> Text of the review

-   **name** -\> Name of the recipe

-   **minutes** -\> Estimate in minutes of how it takes to cook a meal according to the recipe

-   **contributor_id** -\> User ID of who submitted the recipe

-   **submitted** -\> Date recipe was submitted

-   **tags** -\> Tags for the recipe

-   **nutrition** -\> Nutrition information for: number of calories; total fat (PDV); sugar (PDV); sodium (PDV); protein (PDV); saturated fat (PDV); carbohydrates (PDV) - note: PDV means Percent Daily Value

-   **n_steps** -\> Number of steps in the recipe

-   **steps** -\> Text for recipe steps, in order

-   **description** -\> User-provided description.

We'll create a new feature called "word_count" that accounts for the number of words used when writing a review. This will allow us to analyse the numbers of words used in the users' reviews.

Note: We'll look at review text in a more detailed matter in the project below.

```{r}
merged_data$word_count <- lengths(strsplit(merged_data$review, ' '))
average_words <- round(sum(merged_data$word_count)/nrow(merged_data),0)
max_words <- max(merged_data$word_count)

cat(" The average number of words used in a review is", average_words, "\n",
    "The maximum number of words used in a review is", max_words)

```

Let's look at how many reviews the users write.

```{r}
# Count the number of reviews per user_id
user_review_counts <- table(merged_data$user_id)

# Convert to data frame and sort by counts
user_review_counts_df <- data.frame(
  user_id = as.character(names(user_review_counts)),
  review_count = as.numeric(user_review_counts)
)

# Sort the data frame by review_count in descending order
user_review_counts_df <- user_review_counts_df[order(-user_review_counts_df$review_count), ]

# Average and maximum number of reviews
average_n_reviews <- round(sum(user_review_counts_df$review_count)/nrow(user_review_counts_df),1)
max_n_reviews <- max(user_review_counts_df$review_count)

cat(" The average number of reviews written by the same user is", average_n_reviews, "\n",
    "The maximum number of reviews written by the same user is", max_n_reviews)
```

Are there correlated variables?

```{r}
numeric_data <- merged_data[sapply(merged_data, is.numeric)]
cor_matrix <- cor(numeric_data)
#print(cor_matrix) if we want to see a specific value
corrplot(cor_matrix, type = "lower",  tl.srt = 45, title = "Correlation Plot")
```

There seems to be no heavily correlated variables, with the exception of a moderate positive correlation between the number of steps and the number of ingredients in a recipe - The addition of more ingredients to a recipe seems to correlate with an increase in the number of steps required to fulfill the recipe.

Besides that, it's interesting to see that the number of words in a review is not correlated with an user giving a lower or higher rating to a recipe.

We can also explore further on how each type of nutrition may affect a recipe's rating. Currently, the column "nutrition" presents information regarding the different nutritional elements in a confusing way. Let's separate them into different columns.

```{r}
# Separating the different nutritional values into separate columns
final_data <- merged_data %>%
  mutate(nutrition = str_replace_all(nutrition, "\\[|\\]", "")) %>%
  separate(nutrition, into = c("calories_n", "total_fat_pdv", "sugar_pdv", "sodium_pdv", "protein_pdv", "sat_fat_pdv", "carbs_pdv"), sep = ",") %>%
  mutate_at(vars(calories_n, total_fat_pdv, sugar_pdv, sodium_pdv, protein_pdv, sat_fat_pdv, carbs_pdv), as.numeric)
```

```{r}
# Selecting these newly created columns with also the rating attribute
# We also select the name of each recipe in case we want to extract one in the future
nutrition_data <- final_data %>%
  select(name, calories_n, total_fat_pdv, sugar_pdv, sodium_pdv, protein_pdv, sat_fat_pdv, carbs_pdv, rating) %>%
  distinct() %>%
  head(10)

summary(nutrition_data)
```

Now, we can calculate the correlation between each nutritional value with a recipe's rating (for the overall purpose of this project, we're not interested in seeing how each nutritional value correlates with each other).

```{r}
nutrition_data_numeric <- nutrition_data[sapply(nutrition_data, is.numeric)]

# Calculate correlation coefficients between 'rating' and each nutrition variable
correlation_with_rating <- sapply(nutrition_data_numeric, function(x) cor(x, nutrition_data_numeric$rating))

# Create a dataframe to display correlation coefficients
correlation_df <- data.frame(Nutrition_Type = names(correlation_with_rating), Correlation_with_Rating = correlation_with_rating)

# Print the correlation coefficients
print(correlation_df)

```

It's interesting to see how different types of nutrition correlate with a recipe's rating, We see that the amount of sugar and carbohydrates has a moderate negative correlation with a recipe's rating while the other nutritional types display a moderate positive correlation.

But none of these display a significant enough correlation value to leads us into conducting further analysis on this.

Moving on, we'll look at the distribution of recipes' ratings.

```{r}
# Count the frequency of each rating value
rating_freq <- table(final_data$rating)

# Convert the frequency table to a data frame
rating_df <- data.frame(rating = as.numeric(names(rating_freq)),
                        freq = as.numeric(rating_freq))

# Calculate the percentage of each rating
total_ratings <- sum(rating_df$freq)
rating_df$percentage <- (rating_df$freq / total_ratings) * 100

# Plot the frequency and percentage
ggplot(rating_df, aes(x = rating, y = freq)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  geom_text(aes(label = paste0(round(percentage, 1), "%")), vjust = -0.5, size = 3) +
  xlab("Rating") +
  ylab("Frequency") +
  ggtitle("Ratings Frequency and Percentage") +
  theme_minimal()
```

We can see that the rating scores are heavily skewed to ratings of 4 and especially 5. This could mean that people are more inclined to give a review if they had a positive experience when using a recipe.

Before concluding, we'll present some word clouds focused on the recipes' ingredients and on the steps

```{r}
# Ingredients
ingredients <- final_data %>%
  select(ingredients) %>%
  mutate(ingredients = str_replace_all(ingredients, "[\\[\\]']", "")) %>% 
  separate_rows(ingredients, sep = "\\s+") %>%
  filter(ingredients != "") %>%  # Remove empty strings
  count(ingredients) %>%
  filter(!(ingredients %in% c("s", "the", "and", "or", "a")))  # Filter out stop words

suppressWarnings(
  wordcloud(
  words = ingredients$ingredients, 
  freq = ingredients$n, 
  max.words = 200, 
  random.order = FALSE,
  scale = c(3, 0.5),
  colors = brewer.pal(8, "Dark2")
))

# Get the top 5 most used ingredients
top_ingredients <- ingredients %>%
  top_n(5, n) %>%
  arrange(desc(n))

# Print the top 5 most used ingredients as a list
cat("Top 5 most used ingredients:\n")
cat(paste(top_ingredients$ingredients, collapse = ", "))

```

The 5 most used ingredients on recipes from "food.com" are: salt, pepper, sugar, oil and butter. This finding seems pretty reasonable as most traditional dishes requires this ingredients, especially during preparation.

```{r}
# Tags word cloud
tags <- final_data %>%
  select(tags) %>%
  mutate(tags = str_replace_all(tags, "[\\[\\]']", "")) %>%
  separate_rows(tags, sep = "\\s*,\\s*") %>%  # Separates tags by comma and remove spaces
  filter(tags != "") %>%  # Removes empty strings
  count(tags) %>%
  filter(!(tags %in% c("s", "the", "and", "or", "a")))  # Filters out stop words

suppressWarnings(
  wordcloud(
  words = tags$tags, 
  freq = tags$n, 
  max.words = 150, 
  random.order = FALSE,
  scale = c(3, 0.5),
  colors = brewer.pal(8, "Dark2"),
  warn = FALSE 
)
)

# Get the top 5 most used tags
top_tags <- tags %>%
  top_n(5, n) %>%
  arrange(desc(n))

# Print the top 5 most used tags as a list
cat("Top 5 most used tags:\n")
cat(paste(top_tags$tags, collapse = ", "))

```

The 5 most used tags on recipes from "food.com" are preparation, time-to-make, course, dietary and main-ingredient.

The main findings from the exploratory data analysis are:

-   The average and maximum numbers of words used in a review are 54 and 1184, respectively;

-   The average and maximum numbers of reviews written by the same user-id are 5, and 7671, respectively;

-   The addition of more ingredients to a recipe seems to correlate with an increase in the number of steps required to fulfill the recipe, which follows common-sense;

-   The number of words used to write a review of a recipe has almost no correlation with the rating the user gives to the recipe, at least from a simplified approach;

-   Different types of nutrition somewhat correlate with an higher or lower ratings;

-   The recipes' ratings are heavily skewed to higher values, such as 4 or 5, out of a scale of 0 to 5, which potentially indicates that users tend to leave a review on a recipe if it had a good experience using it;

-   The 5 most used ingredients on recipes from "food.com" are: salt, pepper, sugar, oil and butter.

### 2.1. Research Question

Based on the exploratory data analysis, our group decided to pursue the following research question : "is x related to y?"

## 3. Apllying unigrams & bigrams and bigrams & trigrams stemming to the users' reviews to predict recipes' ratings

First step is splitting the data-set

```{r}
#Split the main dataset
set.seed(123) #random split

# Calculate the number of rows to sample for the training set
n_rows <- nrow(final_data)
train_indices <- sample(1:n_rows, size = floor(0.8 * n_rows))

# Create training and test sets
train_set <- final_data[train_indices, ]
test_set <- final_data[-train_indices, ]
```

Based on this, we'll construct two models: - LASSO Model for bigrams and trigrams - LASSO Model for unigrams and bigrams

### 3.1. LASSO Model for unigrams and bigrams

```{r}
'dfm_train_12<-TAB_dfm(train_set$review,ngrams=1:2)
 dfm_test_12<-TAB_dfm(test_set$review,
                           ngrams=1:2,
                           min.prop = 0) %>%
  dfm_match(colnames(dfm_train_12))

saveRDS(dfm_train_12, "dfm_train_12.rds")
saveRDS(dfm_test_12, "dfm_test_12.rds")'


dfm_train_12 <- readRDS("dfm_train_12.rds")
dfm_test_12 <- readRDS("dfm_test_12.rds")
  
lasso_dfm_12<-glmnet::cv.glmnet(x=dfm_train_12, 
                             y=train_set$rating)

# Plot coefficents
plot(lasso_dfm_12, xlab="Log Lambda", ylab="Coefficients", main="LASSO Path", col=rainbow(10))


test_dfm_12_predict <- predict(lasso_dfm_12, newx=dfm_test_12,
                               s='lambda.min')

acc_ngram_12 <- kendall_acc(test_dfm_12_predict, test_set$rating)

saveRDS(acc_ngram_12, "acc_ngram_12.rds")
acc_ngram_12 <- readRDS("acc_ngram_12.rds")


# Plot word map
plotCoefs<-lasso_dfm_12 %>%
  coef(s="lambda.min") %>%
  drop() %>%
  as.data.frame() %>%
  rownames_to_column(var = "ngram") %>%
  rename(score=".") %>%
  filter(score!=0 & ngram!="(Intercept)" & !is.na(score))  

plotFreqs<-data.frame(ngram=colnames(dfm_train_12), freq=colMeans(dfm_train_12))

# We'll be filtering the coefficients based on a low and high threshold to decrease the cluttering in the plot
high_threshold <- 0.1
low_threshold <- -0.1

plotDat_relevant_coefficients <- bind_rows(plotCoefs %>% filter(score > high_threshold), 
                                           plotCoefs %>% filter(score < low_threshold)) %>%
  left_join(plotFreqs) %>%
  mutate_at(vars(score, freq), ~round(., 3))

plotDat_relevant_coefficients %>%
  ggplot(aes(x=score,y=freq,label=ngram,color=score)) +
  scale_color_gradient(low="orange",
                        
                        high="blue")+
  geom_vline(xintercept=0)+
  geom_point() +
  geom_label_repel(max.overlaps =42,
                   point.padding = 0.7,
                   size=2.5,
                   force=6)+  
  scale_y_continuous(trans="log2",
                     breaks=c(-5,.01,.05,.1,.2,.5,1,2,5))+
  scale_x_continuous(limits=c(-0.25,0.25))+
  theme_bw() +
  labs(x="Coefficient",y="Uses per Review")+
  ggtitle("Unigrams and Bigrams Model")+
  theme(legend.position = "none",
        axis.title=element_text(size=12),
        plot.title=element_text(size=12),
        axis.text=element_text(size=8))
```

### 3.2. LASSO Model for bigrams and trigrams

```{r}

'dfm_train_23<-TAB_dfm(train_set$review,ngrams=2:3)

dfm_test_23<-TAB_dfm(test_set$review,
                           ngrams=2:3,
                           min.prop = 0) %>%
  dfm_match(colnames(dfm_train_23))

saveRDS(dfm_train_23, "dfm_train_23.rds")
saveRDS(dfm_test_23, "dfm_test_23.rds")'

dfm_train_23 <- readRDS("dfm_train_23.rds")
dfm_test_23 <- readRDS("dfm_test_23.rds")

lasso_dfm_23<-glmnet::cv.glmnet(x=dfm_train_23, 
                             y=train_set$rating)

# Plot coefficents
plot(lasso_dfm_23, xlab="Log Lambda", ylab="Coefficients", main="LASSO Path", col=rainbow(10))

test_dfm_23_predict <- predict(lasso_dfm_23, newx=dfm_test_23,
                               s='lambda.min')

acc_ngram_23 <- kendall_acc(test_dfm_23_predict, test_set$rating)

saveRDS(acc_ngram_23, "acc_ngram_23.rds")
acc_ngram_23 <- readRDS("acc_ngram_23.rds")


# Plot word map
plotCoefs<-lasso_dfm_23 %>%
  coef(s="lambda.min") %>%
  drop() %>%
  as.data.frame() %>%
  rownames_to_column(var = "ngram") %>%
  rename(score=".") %>%
  filter(score!=0 & ngram!="(Intercept)" & !is.na(score))  

plotDat<-plotCoefs %>%
  left_join(data.frame(ngram=colnames(dfm_train_23),
                       freq=colMeans(dfm_train_23))) %>%
  mutate_at(vars(score,freq),~round(.,3))

plotDat %>%
  ggplot(aes(x=score,y=freq,label=ngram,color=score)) +
  scale_color_gradient(low="orange",
                        
                        high="blue")+
  geom_vline(xintercept=0)+
  geom_point() +
  geom_label_repel(max.overlaps =42,
                   point.padding = 0.7,
                   size=2.5,
                   force=6)+  
  scale_y_continuous(trans="log2",
                     breaks=c(-5,.01,.05,.1,.2,.5,1,2,5))+
  scale_x_continuous(limits=c(-0.25,0.25))+
  theme_bw() +
  labs(x="Coefficient",y="Uses per Review")+
  ggtitle("Bigrams and Trigrams Model")+
  theme(legend.position = "none",
        axis.title=element_text(size=12),
        plot.title=element_text(size=12),
        axis.text=element_text(size=8))
```

We aren't happy with our models performance so we'll try another approach: predicting if it's a negative or a positive review. We'll be considering a review to be negative if the rating is 3 or lower and to be positive if it's 4 or 5. The high skedweness of the ratings distribution towards higher ratings made us choose this threshold instead of the median.

```{r}
# Let's start by creating our binary rating - This is only in case we want to do a new split in the future
final_data_binary <- final_data
final_data_binary$binary <- ifelse(final_data_binary$rating <= 3, 0, 1 )

# Doing the same for the training and testing data sets
train_set_binary <- train_set
train_set_binary$binary <- ifelse(train_set_binary$rating <= 3, 0, 1 )

test_set_binary <- test_set
test_set_binary$binary <- ifelse(test_set_binary$rating <= 3, 0, 1 )

```

### 3.3. Binary LASSO Model for unigrams and bigrams

```{r}
'dfm_train_12_binary<-TAB_dfm(train_set_binary$review,ngrams=1:2)
 dfm_test_12_binary<-TAB_dfm(test_set_binary$review,
                           ngrams=1:2,
                           min.prop = 0) %>%
  dfm_match(colnames(dfm_train_12_binary))

saveRDS(dfm_train_12_binary, "dfm_train_12_binary.rds")
saveRDS(dfm_test_12_binary, "dfm_test_12_binary.rds")'


dfm_train_12_binary <- readRDS("dfm_train_12_binary.rds")
dfm_test_12_binary <- readRDS("dfm_test_12_binary.rds")
  
lasso_dfm_12_binary<-glmnet::cv.glmnet(x=dfm_train_12_binary, 
                             y=train_set_binary$rating)

# Plot coefficents
plot(lasso_dfm_12_binary, xlab="Log Lambda", ylab="Coefficients", main="LASSO Path", col=rainbow(10))

test_dfm_12_binary_predict <- predict(lasso_dfm_12_binary, newx=dfm_test_12_binary,
                               s='lambda.min')

acc_ngram_12_binary <- kendall_acc(test_dfm_12_binary_predict, test_set_binary$binary)

saveRDS(acc_ngram_12_binary, "acc_ngram_12_binary.rds")
acc_ngram_12_binary <- readRDS("acc_ngram_12_binary.rds")


# Plot word map
plotCoefs<-lasso_dfm_12_binary %>%
  coef(s="lambda.min") %>%
  drop() %>%
  as.data.frame() %>%
  rownames_to_column(var = "ngram") %>%
  rename(score=".") %>%
  filter(score!=0 & ngram!="(Intercept)" & !is.na(score))  

plotFreqs<-data.frame(ngram=colnames(lasso_dfm_12_binary), freq=colMeans(lasso_dfm_12_binary))

# We'll be filtering the coefficients based on a low and high threshold to decrease the cluttering in the plot
high_threshold <- 0.1
low_threshold <- -0.1

plotDat_relevant_coefficients <- bind_rows(plotCoefs %>% filter(score > high_threshold), 
                                           plotCoefs %>% filter(score < low_threshold)) %>%
  left_join(plotFreqs) %>%
  mutate_at(vars(score, freq), ~round(., 3))

plotDat_relevant_coefficients %>%
  ggplot(aes(x=score,y=freq,label=ngram,color=score)) +
  scale_color_gradient(low="orange",
                        
                        high="blue")+
  geom_vline(xintercept=0)+
  geom_point() +
  geom_label_repel(max.overlaps =42,
                   point.padding = 0.7,
                   size=2.5,
                   force=6)+  
  scale_y_continuous(trans="log2",
                     breaks=c(-5,.01,.05,.1,.2,.5,1,2,5))+
  scale_x_continuous(limits=c(-0.25,0.25))+
  theme_bw() +
  labs(x="Coefficient",y="Uses per Review")+
  ggtitle("Binary Unigrams and Bigrams Model")+
  theme(legend.position = "none",
        axis.title=element_text(size=12),
        plot.title=element_text(size=12),
        axis.text=element_text(size=8))
```

### 3.4. Binary LASSO Model for bigrams and trigrams

```{r}
'dfm_train_23_binary<-TAB_dfm(train_set_binary$review,ngrams=2:3)

dfm_test_23_binary<-TAB_dfm(test_set_binary$review,
                           ngrams=2:3,
                           min.prop = 0) %>%
  dfm_match(colnames(dfm_train_23_binary))

saveRDS(dfm_train_23_binary, "dfm_train_23_binary.rds")
saveRDS(dfm_test_23_binary, "dfm_test_23_binary.rds")'

dfm_train_23_binary <- readRDS("dfm_train_23_binary.rds")
dfm_test_23_binary <- readRDS("dfm_test_23_binary.rds")

lasso_dfm_23_binary<-glmnet::cv.glmnet(x=dfm_train_23_binary, 
                             y=train_set_binary$rating)

# Plot coefficents
plot(lasso_dfm_23_binary, xlab="Log Lambda", ylab="Coefficients", main="LASSO Path", col=rainbow(10))


test_dfm_23_binary_predict <- predict(lasso_dfm_23_binary, newx=dfm_test_23_binary,
                               s='lambda.min')

acc_ngram_23_binary <- kendall_acc(test_dfm_23_binary_predict, test_set_binary$rating)

saveRDS(acc_ngram_23_binary, "acc_ngram_23_binary.rds")
acc_ngram_23_binary <- readRDS("acc_ngram_23_binary.rds")

# Plot word map
plotCoefs<-lasso_dfm_23_binary %>%
  coef(s="lambda.min") %>%
  drop() %>%
  as.data.frame() %>%
  rownames_to_column(var = "ngram") %>%
  rename(score=".") %>%
  filter(score!=0 & ngram!="(Intercept)" & !is.na(score))  

plotDat<-plotCoefs %>%
  left_join(data.frame(ngram=colnames(dfm_train_23_binary),
                       freq=colMeans(dfm_train_23_binary))) %>%
  mutate_at(vars(score,freq),~round(.,3))

plotDat %>%
  ggplot(aes(x=score,y=freq,label=ngram,color=score)) +
  scale_color_gradient(low="orange",
                        
                        high="blue")+
  geom_vline(xintercept=0)+
  geom_point() +
  geom_label_repel(max.overlaps =42,
                   point.padding = 0.7,
                   size=2.5,
                   force=6)+  
  scale_y_continuous(trans="log2",
                     breaks=c(-5,.01,.05,.1,.2,.5,1,2,5))+
  scale_x_continuous(limits=c(-0.25,0.25))+
  theme_bw() +
  labs(x="Coefficient",y="Uses per Review")+
  ggtitle("Bigrams and Trigrams Model for binary rating")+
  theme(legend.position = "none",
        axis.title=element_text(size=12),
        plot.title=element_text(size=12),
        axis.text=element_text(size=8))
```

## 4. Aplying unigrams & bigrams stemming to users' reviews and to recipes' descriptions, steps and ingredients to predict recipes' ratings

### Description

```{r}
# unigrams and bigrams model on description

'dfm_train_12_description<-TAB_dfm(train_set$description,ngrams=1:2)
dfm_test_12_description<-TAB_dfm(test_set$description,
                           ngrams=1:2,
                           min.prop = 0) %>%
  dfm_match(colnames(dfm_train_12_description))

saveRDS(dfm_train_12_description, "dfm_train_12_description.rds")
saveRDS(dfm_test_12_description, "dfm_test_12_description.rds")'

dfm_train_12_description <- readRDS("dfm_train_12_description.rds")
dfm_test_12_description <- readRDS("dfm_test_12_description.rds")
  
lasso_dfm_12_description<-glmnet::cv.glmnet(x=dfm_train_12_description, 
                             y=train_set$rating)

# Plot coefficents
plot(lasso_dfm_12_description, xlab="Log Lambda", ylab="Coefficients", main="LASSO Path", col=rainbow(10))


# ngram model on description
plot(lasso_dfm_12_description)

test_dfm_12_predict_description<-predict(lasso_dfm_12_description,newx = dfm_test_12_description,
                          s="lambda.min")

kendall_acc(test_dfm_12_predict_description,test_set$rating)


# Plot word map
plotCoefs<-lasso_dfm_12_description %>%
  coef(s="lambda.min") %>%
  drop() %>%
  as.data.frame() %>%
  rownames_to_column(var = "ngram") %>%
  rename(score=".") %>%
  filter(score!=0 & ngram!="(Intercept)" & !is.na(score))  

plotFreqs<-data.frame(ngram=colnames(dfm_train_12_description), freq=colMeans(dfm_train_12_description))
```

```{r}
# We'll be filtering the coefficients based on a low and high threshold to decrease the cluttering in the plot
high_threshold <- 0.03
low_threshold <- -0.03

plotDat_relevant_coefficients <- bind_rows(plotCoefs %>% filter(score > high_threshold), 
                                           plotCoefs %>% filter(score < low_threshold)) %>%
  left_join(plotFreqs) %>%
  mutate_at(vars(score, freq), ~round(., 3))

plotDat_relevant_coefficients %>%
  ggplot(aes(x=score,y=freq,label=ngram,color=score)) +
  scale_color_gradient(low="orange",
                        
                        high="blue")+
  geom_vline(xintercept=0)+
  geom_point() +
  geom_label_repel(max.overlaps =42,
                   point.padding = 0.7,
                   size=2.5,
                   force=6)+  
  scale_y_continuous(trans="log2",
                     breaks=c(-5,.01,.05,.1,.2,.5,1,2,5))+
  scale_x_continuous(limits=c(-0.25,0.25))+
  theme_bw() +
  labs(x="Coefficient",y="Uses per Description")+
  ggtitle("Unigrams and Bigrams Model")+
  theme(legend.position = "none",
        axis.title=element_text(size=12),
        plot.title=element_text(size=12),
        axis.text=element_text(size=8))
```

### Steps

```{r}
# unigrams and bigrams model on steps

'dfm_train_12_steps <-TAB_dfm(train_set$steps,ngrams=1:2)
dfm_test_12_steps<-TAB_dfm(test_set$steps,
                           ngrams=1:2,
                           min.prop = 0) %>%
  dfm_match(colnames(dfm_train_12_steps))

saveRDS(dfm_train_12_steps, "dfm_train_12_steps.rds")
saveRDS(dfm_test_12_steps, "dfm_test_12_steps.rds")'

dfm_train_12_steps <- readRDS("dfm_train_12_steps.rds")
dfm_test_12_steps <- readRDS("dfm_test_12_steps.rds")
  
lasso_dfm_12_steps<-glmnet::cv.glmnet(x=dfm_train_12_steps, 
                             y=train_set$rating)

plot(lasso_dfm_12_steps)

test_dfm_12_predict_steps<-predict(lasso_dfm_12_steps,newx = dfm_test_12_steps,
                          s="lambda.min")

kendall_acc(test_dfm_12_predict_steps,test_set$rating)


# Plot coefficents
plot(lasso_dfm_12_steps, xlab="Log Lambda", ylab="Coefficients", main="LASSO Path", col=rainbow(10))


# Plot word map
plotCoefs<-lasso_dfm_12_steps %>%
  coef(s="lambda.min") %>%
  drop() %>%
  as.data.frame() %>%
  rownames_to_column(var = "ngram") %>%
  rename(score=".") %>%
  filter(score!=0 & ngram!="(Intercept)" & !is.na(score))  

plotFreqs<-data.frame(ngram=colnames(dfm_train_12_steps), freq=colMeans(dfm_train_12_steps))
```

```{r}
# We'll be filtering the coefficients based on a low and high threshold to decrease the cluttering in the plot
high_threshold <- 0.04
low_threshold <- -0.04

plotDat_relevant_coefficients <- bind_rows(plotCoefs %>% filter(score > high_threshold), 
                                           plotCoefs %>% filter(score < low_threshold)) %>%
  left_join(plotFreqs) %>%
  mutate_at(vars(score, freq), ~round(., 3))

plotDat_relevant_coefficients %>%
  ggplot(aes(x=score,y=freq,label=ngram,color=score)) +
  scale_color_gradient(low="orange",
                        
                        high="blue")+
  geom_vline(xintercept=0)+
  geom_point() +
  geom_label_repel(max.overlaps =42,
                   point.padding = 0.7,
                   size=2.5,
                   force=6)+  
  scale_y_continuous(trans="log2",
                     breaks=c(-5,.01,.05,.1,.2,.5,1,2,5))+
  scale_x_continuous(limits=c(-0.25,0.25))+
  theme_bw() +
  labs(x="Coefficient",y="Uses per Step List")+
  ggtitle("Unigrams and Bigrams Model")+
  theme(legend.position = "none",
        axis.title=element_text(size=12),
        plot.title=element_text(size=12),
        axis.text=element_text(size=8))
```

### Ingredients

```{r}
# unigrams and bigrams model on ingredients

'dfm_train_12_ingredients <-TAB_dfm(train_set$ingredients,ngrams=1:2)
dfm_test_12_ingredients<-TAB_dfm(test_set$ingredients,
                           ngrams=1:2,
                           min.prop = 0) %>%
  dfm_match(colnames(dfm_train_12_ingredients))

saveRDS(dfm_train_12_ingredients, "dfm_train_12_ingredients.rds")
saveRDS(dfm_test_12_ingredients, "dfm_test_12_ingredients.rds")'

dfm_train_12_ingredients <- readRDS("dfm_train_12_ingredients.rds")
dfm_test_12_ingredients <- readRDS("dfm_test_12_ingredients.rds")
  
lasso_dfm_12_ingredients<-glmnet::cv.glmnet(x=dfm_train_12_ingredients, 
                             y=train_set$rating)

plot(lasso_dfm_12_ingredients)

test_dfm_12_predict_ingredients<-predict(lasso_dfm_12_ingredients,newx = dfm_test_12_ingredients,
                          s="lambda.min")

kendall_acc(test_dfm_12_predict_ingredients,test_set$rating)


# Plot coefficents
plot(lasso_dfm_12_ingredients, xlab="Log Lambda", ylab="Coefficients", main="LASSO Path", col=rainbow(10))


# Plot word map
plotCoefs<-lasso_dfm_12_ingredients %>%
  coef(s="lambda.min") %>%
  drop() %>%
  as.data.frame() %>%
  rownames_to_column(var = "ngram") %>%
  rename(score=".") %>%
  filter(score!=0 & ngram!="(Intercept)" & !is.na(score))  

plotFreqs<-data.frame(ngram=colnames(dfm_train_12_ingredients), freq=colMeans(dfm_train_12_ingredients))
```

```{r}
# We'll be filtering the coefficients based on a low and high threshold to decrease the cluttering in the plot
high_threshold <- 0.05
low_threshold <- -0.05

plotDat_relevant_coefficients <- bind_rows(plotCoefs %>% filter(score > high_threshold), 
                                           plotCoefs %>% filter(score < low_threshold)) %>%
  left_join(plotFreqs) %>%
  mutate_at(vars(score, freq), ~round(., 3))

plotDat_relevant_coefficients %>%
  ggplot(aes(x=score,y=freq,label=ngram,color=score)) +
  scale_color_gradient(low="orange",
                        
                        high="blue")+
  geom_vline(xintercept=0)+
  geom_point() +
  geom_label_repel(max.overlaps =42,
                   point.padding = 0.7,
                   size=2.5,
                   force=6)+  
  scale_y_continuous(trans="log2",
                     breaks=c(-5,.01,.05,.1,.2,.5,1,2,5))+
  scale_x_continuous(limits=c(-0.25,0.25))+
  theme_bw() +
  labs(x="Coefficient",y="Uses per Ingredient")+
  ggtitle("Unigrams and Bigrams Model")+
  theme(legend.position = "none",
        axis.title=element_text(size=12),
        plot.title=element_text(size=12),
        axis.text=element_text(size=8))
```

### Accuracy plots

```{r}
# Accuracy plot with confidence intervals

# Accuracy scores and their confidence intervals
model_data <- data.frame(
  Model = c("DFM review", "DFM description", "DFM steps", "DFM ingredients"),
  Accuracy = c(60.95, 52.04, 53.31, 53.12),
  Lower = c(60.75, 51.83, 53.11, 52.91),
  Upper = c(61.15, 52.24, 53.52, 53.32)
)

# Plotting the accuracies with confidence intervals
accuracy_plot <- ggplot(model_data, aes(x = Model, y = Accuracy, fill = Model)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  geom_errorbar(aes(ymin = Lower, ymax = Upper), width = 0.2, position = position_dodge(0.9)) +
  geom_text(aes(label = round(Accuracy, 2)), vjust = 2) + # adjust vjust here
  scale_fill_brewer(palette = "Pastel1") +
  labs(title = "Accuracy Scores of DFM Models",
       x = "DFM Model", y = "Accuracy (%)") +
  theme_minimal() +
  theme(legend.position = "none", axis.text.x = element_text(angle = 45, hjust = 1))

# Print the plot
print(accuracy_plot)

```

## 5. Benchmarks

```{r}
test_set <- test_set %>%
  mutate(speech_sent=syuzhet::get_sentiment(review),
         speech_wdct=str_count(review,"[[:alpha:]]+"))

acc_wdct<-kendall_acc(test_set$rating,test_set$speech_wdct)

saveRDS(acc_wdct, "acc_wdct.rds")
acc_wdct <- readRDS("acc_wdct.rds")

acc_sent<-kendall_acc(test_set$rating,test_set$speech_sent)

saveRDS(acc_sent, "acc_sent.rds")
acc_sent <- readRDS("acc_sent.rds")

# Plot
bind_rows(acc_ngram_12 %>%
            mutate(model="Unigrams and Bigrams"),
          acc_ngram_23%>%
            mutate(model="Bigrams and Trigrams"),
          acc_wdct %>%
            mutate(model="Word Count"),
          acc_sent %>%
            mutate(model="Traditional Sentiment")
) %>% 
  ggplot(aes(x=model,color=model,
             y=acc,ymin=lower,ymax=upper)) +
  geom_hline(yintercept=50) +              
  geom_point() +                           
  geom_errorbar(width=.4) +                
  labs(x="Model",                 
       y="Accuracy") +  
  coord_flip() + 
  theme_bw() +                             
  theme(axis.text=element_text(size=20),
        axis.title=element_text(size=24),
        panel.grid = element_blank(),
        strip.text=element_text(size=24),
        strip.background = element_rect(fill="white"),
        legend.position = "none")          # other design options
```

# JACOB's benchmarks

Dictionary Models as benchmarks

```{r}
# First we create a smaller subset so it doesnt run forever
# Set seed for reproducibility
set.seed(123)

# Calculate the number of rows to sample for the subset
n_rows_train = nrow(train_set)
subset_indices = sample(1:n_rows_train, size = 20000)  # Adjust the size as needed

# Create the smaller subset from the training set
train_subset = train_set[subset_indices, ]
```

```{r}
library(textdata)

loughran_words = textdata::lexicon_loughran()

# extract dictionary
positive_dict<-loughran_words %>%
  filter(sentiment=="positive") %>%
  pull(word)

# collapse into a "document"
positive_dict_doc<-positive_dict %>%
  paste(collapse=" ")
```

```{r}
#############################################
# extract dictionary the normal way
#############################################

# Traditional dictionary approach using dfm_lookup()
train_dicts<-train_subset %>%
  pull(review) %>%
  tokens() %>%
  dfm() %>%
  dfm_lookup(as.dictionary(loughran_words)) %>%
  convert(to="data.frame")


# all the dictionaries are in there!
head(train_dicts)
```

```{r}
# Accuracy score using positive sentiment
acc_dict_p <- kendall_acc(train_dicts$positive,
            train_subset$rating)

# Accuracy score using uncertainty sentiment
acc_dict_u <- kendall_acc(train_dicts$uncertainty,
            train_subset$rating)

# Accuracy score using negative sentiment
acc_dict_n <- kendall_acc(train_dicts$negative,
            train_subset$rating)
```

```{r}
########################################################
# using L&M dictionary with grammar awareness
########################################################

train_dicts<-train_dicts %>%
  mutate(sentiment=positive-negative)

train_subset<-train_subset %>%
  mutate(LMsentiment=sentiment_by(review,
                                  polarity_dt=lexicon::hash_sentiment_loughran_mcdonald) %>%
           pull(ave_sentiment))

acc_lnm <- kendall_acc(train_subset$LMsentiment,
            train_subset$rating)
```

### Do it for the plot

```{r}

bind_rows(acc_ngram_12 %>%
            mutate(model="Unigrams and Bigrams"),
          acc_ngram_23 %>%
            mutate(model="Bigrams and Trigrams"),
          acc_wdct %>%
            mutate(model="Word Count"),
          acc_sent %>%
            mutate(model="Traditional Sentiment"),
          acc_dict_p %>%
            mutate(model="Traditional Possitive Dictionary"),
          acc_dict_u %>%
            mutate(model="Traditional Uncertanty Dictionary"),
          acc_dict_n %>%
            mutate(model="Traditional Negative Dictionary"),
          acc_lnm %>%
            mutate(model="LM Dictionary with Grammar Awareness")
) %>% 
  ggplot(aes(x=model,color=model,
             y=acc,ymin=lower,ymax=upper)) +
  geom_hline(yintercept=50) +              
  geom_point() +                           
  geom_errorbar(width=.4) +                
  labs(x="Model",                 
       y="Accuracy") +  
  coord_flip() + 
  theme_bw() +                             
  theme(axis.text=element_text(size=11),
        axis.title=element_text(size=16),
        panel.grid = element_blank(),
        strip.text=element_text(size=24),
        strip.background = element_rect(fill="white"),
        legend.position = "none")          # other design options

```

## 6. Predicting if a user is going to write a positive or a negative review (same approach as before)

```{r}

```

## 7. Transfer learning - training a model on positive reviews to predict negative reviews and vice-versa

```{r}
# Positive split

positive<-final_data_binary %>%
  filter(binary==1)

# Calculate the number of rows to sample for the training set for positive review data
p_rows <- nrow(positive)
train_indices_pos <- sample(1:p_rows, size = floor(0.8 * p_rows))

# Create training and test sets
positive_train <- positive[train_indices_pos, ]
positive_test <- positive[-train_indices_pos, ]

# Negative split

negative<-final_data_binary %>%
  filter(binary==0)

# Calculate the number of rows to sample for the training set for negative review data
n_rows <- nrow(negative)
train_indices_neg <- sample(1:p_rows, size = floor(0.8 * p_rows))

# Create training and test sets
negative_train <- negative[train_indices_neg, ]
negative_test <- negative[-train_indices_neg, ]

# create our prediction variables from the cons text
dfm_train_neg<-TAB_dfm(negative_train$review,ngrams=1:2) %>%
  convert(to="matrix")

train_neg_model<-cv.glmnet(x=dfm_train_neg,
                                y=negative_train$review)

dfm_test_neg <-TAB_dfm(negative_test$review,ngrams=1:2)  %>%
  dfm_match(colnames(dfm_train_neg)) %>%
  convert(to="matrix")

# generate predictions for test data
test_predict<-predict(train_neg_model,
                                     newx = dfm_train_neg)[,1]
# estimate accuracy
neg_acc<-kendall_acc(test_predict,positive_test$review)

dfm_microsoft_test_cons<-TAB_dfm(gd_microsoft_test$cons,ngrams=1:2)  %>%
  dfm_match(colnames(dfm_amazon_train_cons)) %>%
  convert(to="matrix")

# generate predictions for test data
microsoft_test_predict_amzn<-predict(amazon_model_cons,
                                     newx = dfm_microsoft_test_cons)[,1]
# estimate accuracy
pos_acc <-kendall_acc(microsoft_test_predict_amzn,microsoft_test_Y)


neg_acc

pos_acc
```

Below, is code from assignment 2 that we can copy for the transfer learning part:

```{r}
gd_microsoft_train<-gd_train %>%
  filter(company=="microsoft")

gd_microsoft_test<-gd_test %>%
  filter(company=="microsoft")

microsoft_train_Y<-gd_microsoft_train %>%
  pull(overall)

microsoft_test_Y<-gd_microsoft_test %>%
  pull(overall)


# create our prediction variables from the cons text
dfm_microsoft_train_cons<-TAB_dfm(gd_microsoft_train$cons,ngrams=1:2) %>%
  convert(to="matrix")

microsoft_model_cons<-cv.glmnet(x=dfm_microsoft_train_cons,
                                y=microsoft_train_Y)

dfm_microsoft_test_cons<-TAB_dfm(gd_microsoft_test$cons,ngrams=1:2)  %>%
  dfm_match(colnames(dfm_microsoft_train_cons)) %>%
  convert(to="matrix")

# generate predictions for test data
microsoft_test_predict_msft<-predict(microsoft_model_cons,
                                     newx = dfm_microsoft_test_cons)[,1]
# estimate accuracy
cons_acc_msft<-kendall_acc(microsoft_test_predict_msft,microsoft_test_Y)


dfm_microsoft_test_cons<-TAB_dfm(gd_microsoft_test$cons,ngrams=1:2)  %>%
  dfm_match(colnames(dfm_amazon_train_cons)) %>%
  convert(to="matrix")

# generate predictions for test data
microsoft_test_predict_amzn<-predict(amazon_model_cons,
                                     newx = dfm_microsoft_test_cons)[,1]
# estimate accuracy
cons_acc_amzn<-kendall_acc(microsoft_test_predict_amzn,microsoft_test_Y)


cons_acc_msft

cons_acc_amzn
```

## 8. Politeness on Review Data

```{r}
#Reducing rows to run spacy faster
train_indices_spacy <- sample(1:n_rows, size = 100000)

# Create training and test sets with lowered no of rows
train_set_spacy <- final_data[train_indices_spacy, ]
train_set_spacy <- train_set_spacy %>%
  mutate(rating_category = ifelse(rating >= 0 & rating <= 3, "Mostly Negative Reviewed Recipe", "Mostly Positive Review Recipe"))
test_set_spacy <- final_data[-train_indices_spacy, ]
test_set_spacy <- test_set_spacy %>%
  mutate(rating_category = ifelse(rating >= 0 & rating <= 3,"Mostly Negative Reviewed Recipe", "Mostly Positive Review Recipe"))

# 
# spacyr::spacy_initialize()
# parsed_reviews <- spacy_parse(train_set_spacy$review, entity = TRUE)
# 
# # View entities extracted from reviews
# head(parsed_reviews$entity)


# reviews_politeness<-politeness(train_set_spacy$review,parser="spacy")
# saveRDS(reviews_politeness,"reviews_politeness.Rds")
reviews_politeness <- readRDS("reviews_politeness.Rds")
politenessPlot(reviews_politeness,
               train_set_spacy$rating_category,
               middle_out = .05)

ggsave("Politeness Plot.png", height = 12, width = 15)
```

## 9. Limitations and improvements
